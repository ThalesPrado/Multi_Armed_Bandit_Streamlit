import streamlit as st

# üõ†Ô∏è Configura√ß√£o da p√°gina (sempre a primeira chamada do Streamlit)
st.set_page_config(page_title="Bandits para Otimiza√ß√£o de Pre√ßos", layout="wide")

st.title("üìò Teoria, Formula√ß√£o e Objetivo")

# Objetivo
st.markdown("""
## üéØ Objetivo do Projeto

O objetivo com a aplica√ß√£o de algoritmos **Multi-Armed Bandit (MAB)** √© **otimizar os pre√ßos de venda de produtos** (SKU), maximizando o **lucro ou a receita**, enquanto aprendemos continuamente com o comportamento real dos consumidores.
""")

# Intui√ß√£o
st.markdown("---")
st.markdown("### üß† Intui√ß√£o: M√°quina Ca√ßa-N√≠quel")

st.markdown("""
Imagine que voc√™ est√° em um cassino com v√°rias m√°quinas ca√ßa-n√≠queis (slot machines). Cada m√°quina (ou bra√ßo) te d√° um pagamento (recompensa), mas com uma distribui√ß√£o aleat√≥ria e desconhecida.

Cada **pre√ßo** √© como uma alavanca de uma m√°quina ca√ßa-n√≠quel. Ao escolher um pre√ßo, voc√™ observa uma recompensa (lucro) associada √† venda a esse pre√ßo.

Nosso desafio √© o **trade-off** entre:
- **Explorar**: testar outros pre√ßos para descobrir novas oportunidades.
- **Explorar o melhor conhecido**: usar o que j√° sabemos para maximizar o lucro.
""")

# Algoritmo
st.markdown("---")
st.markdown("## ‚öôÔ∏è Algoritmo")

st.markdown("### 1. Multi-Armed Bandit (Nota√ß√£o Formal)")
st.latex(r"A = \{a_1, a_2, ..., a_K\}")
st.latex(r"r_k \sim \mathcal{D}_k")
st.markdown("""
- Cada a√ß√£o ak representa um pre√ßo.
- rk: recompensa observada ao escolher o pre√ßo ak.
- Dk: distribui√ß√£o de probabilidade (desconhecida) da recompensa.
""")

# Fun√ß√£o Objetivo
st.markdown("---")
st.markdown("### 2. Fun√ß√£o Objetivo")

st.markdown("Queremos **maximizar a soma esperada das recompensas** ao longo das intera√ß√µes:")

st.latex(r"\text{Maximize} \quad \sum_{t=1}^{T} \mathbb{E}[r_t]")

st.markdown("""
**Onde:**
- T: n√∫mero total de rodadas (intera√ß√µes).
- rt: recompensa recebida na rodada \( t \).
""")

# Regret
st.markdown("---")
st.markdown("### Equivalentemente: Minimizar o *Regret*")

st.markdown("Como n√£o sabemos de antem√£o qual a√ß√£o √© a melhor, expressamos o objetivo como:")

st.latex(r"R_T = T \cdot \mu^* - \sum_{t=1}^{T} \mathbb{E}[r_t]")

st.markdown("""
**Onde:**
- mi*: maior recompensa esperada entre todas as a√ß√µes.
- RT: arrependimento (regret) acumulado at√© o tempo T.
""")

st.latex(r"""
\boxed{
\text{Maximize } \sum_{t=1}^{T} \mathbb{E}[r_t] \quad \text{ou} \quad \text{Minimize } R_T
}
""")

# Processo de decis√£o
st.markdown("---")
st.markdown("### 3. Processo de Decis√£o")

st.markdown(r"""
A cada rodada t:
1. Escolhemos um pre√ßo ak.
2. Observamos a recompensa rt (ex: lucro).
3. Atualizamos nossa estimativa para aquele pre√ßo.  
""")

# UCB
st.markdown("---")
st.markdown("## 4. üìà Algoritmo UCB (Upper Confidence Bound)")

st.markdown("""
O algoritmo UCB escolhe o pre√ßo com a maior soma entre:
- M√©dia observada at√© agora.
- Um b√¥nus de incerteza que favorece pre√ßos pouco testados.
""")

st.latex(r"a_t = \arg\max_a \left[ \hat{\mu}_a + \sqrt{\frac{2\log t}{n_a}} \right]")

st.markdown("""
- mi_chapeu_a: m√©dia da recompensa observada para o pre√ßo a  
- na: n√∫mero de vezes que o pre√ßo foi testado  
- t: rodada atual  
""")

# 5 - epsilon-greedy
st.markdown("## 5. üìà Algoritmo Epsilon-Greedy")

st.markdown("""
O algoritmo **Epsilon-Greedy** busca equilibrar **explora√ß√£o** e **explora√ß√£o do melhor pre√ßo conhecido** da seguinte forma:

- Com probabilidade Œµ, escolhe uma a√ß√£o (pre√ßo) aleatoriamente (**explora√ß√£o**)  
- Com probabilidade 1 - Œµ, escolhe o pre√ßo com maior lucro m√©dio estimado (**explora√ß√£o do melhor**)  
""")

st.latex(r"""
a_t =
\begin{cases}
\text{a√ß√£o aleat√≥ria}, & \text{com probabilidade } \epsilon \\
\arg\max_a \hat{\mu}_a, & \text{com probabilidade } 1 - \epsilon
\end{cases}
""")

st.markdown("""
- Œµ: taxa de explora√ß√£o (ex: 0.1 = 10%)
- mi_chapeu_a: m√©dia da recompensa observada para a a√ß√£o a

Ap√≥s observar o lucro Ri,t ao testar o pre√ßo pi, o algoritmo atualiza sua estimativa de lucro m√©dio Q(pi) da seguinte forma:
""")

st.latex(r"""
Q(p_i) \leftarrow Q(p_i) + \frac{1}{N(p_i)} \left( r_{i,t} - Q(p_i) \right)
""")

st.markdown("""
- Q(pi): estimativa atual do lucro m√©dio para o pre√ßo pi
- Ri,t: lucro observado na rodada t com pre√ßo pi
- N(pi): n√∫mero de vezes que o pre√ßo pi foi selecionado

Essa f√≥rmula permite que o algoritmo aprenda progressivamente qual pre√ßo √© mais lucrativo sem precisar armazenar todo o hist√≥rico de vendas.
""")


# 6 - Thompson Sampling
st.markdown("## 6. üìà Algoritmo Thompson Sampling")

st.markdown("""
O algoritmo **Thompson Sampling** √© uma abordagem Bayesiana que equilibra automaticamente explora√ß√£o e explora√ß√£o.


A cada rodada:
- Ele sorteia um valor de recompensa (lucro) esperado para cada pre√ßo com base nas observa√ß√µes anteriores.
- Seleciona o pre√ßo que teve a maior amostra sorteada.
- Ap√≥s a venda, atualiza suas estat√≠sticas, refinando a estimativa futura para aquele pre√ßo.

Essa estrat√©gia favorece pre√ßos promissores, mas ainda testa pre√ßos com alta incerteza, o que √© essencial em cen√°rios com dados limitados.

""")

st.latex(r"""
\tilde{\mu}_i \sim \mathcal{N}(\hat{\mu}_i, \hat{\sigma}_i)
""")

st.markdown("""
- mi_chapeu_i: m√©dia dos lucros obtidos com o pre√ßo pi
- sigma_chapeu_i: desvio padr√£o (incerteza) do lucro para aquele pre√ßo

### Escolha da a√ß√£o:

""")

st.latex(r"""
a_t = \arg\max_i \tilde{\mu}_i
""")

st.markdown("""
Ou seja, testamos o **pre√ßo com a melhor amostra sorteada**.

### Atualiza√ß√£o ap√≥s nova venda:

Ap√≥s observar o lucro ri,t ao aplicar o pre√ßo pi, atualizamos:

- N√∫mero de testes N(pi)
- Soma dos lucros S(pi)
- Soma dos quadrados dos lucros SS(pi)

E assim recalculamos:

""")

st.latex(r"""
\hat{\mu}_i = \frac{S(p_i)}{N(p_i)}, \quad
\hat{\sigma}_i = \sqrt{\frac{SS(p_i)}{N(p_i)} - \hat{\mu}_i^2 + \epsilon}
""")


st.markdown("""
O pequeno termo Œµ evita problemas com vari√¢ncias negativas nos primeiros testes.
""")


# Compara√ß√£o com Regress√£o
st.markdown("---")
st.markdown("## 7. üìä Compara√ß√£o com Modelos de Regress√£o")

st.markdown("""
| Caracter√≠stica       | Multi-Armed Bandit (MAB) | Regress√£o Tradicional | Principais diferen√ßas |
|----------------------|---------------------------|------------------------|---------------------|
| **Tipo de problema** | Otimiza√ß√£o sequencial    | Previs√£o / infer√™ncia  | Bandits tomam decis√µes em sequ√™ncia para maximizar recompensa. Regress√£o prev√™ valores com base em vari√°veis. |
| **Feedback**         | Parcial (s√≥ v√™ a recompensa da a√ß√£o escolhida) | Completo (v√™ todos os alvos) | Em MAB, voc√™ s√≥ v√™ o lucro do pre√ßo testado. Na regress√£o, voc√™ conhece todos os alvos. |
| **Explora√ß√£o**       | Sim (via epsilon-Greedy, UCB) | N√£o (base fixa) | Bandits testam novas a√ß√µes mesmo sem certeza. Regress√£o s√≥ aprende com dados que j√° possui. |
| **Atualiza√ß√£o**      | Online (a cada nova intera√ß√£o) | Offline (batch completo) | MAB aprende em tempo real. Regress√£o precisa reprocessar toda a base para atualizar. |
| **Fun√ß√£o objetivo**  | Maximizar soma de recompensas ou minimizar o arrependimento (regret) | Minimizar erro entre previs√£o e valor real (erro quadr√°tico m√©dio) | Bandits otimizam decis√µes, regress√£o minimiza diferen√ßa entre previsto e real. |
| **Suposi√ß√µes**       | N√£o assume forma funcional dos dados | Assume forma funcional (linearidade, normalidade, etc.) | Bandits s√£o mais flex√≠veis. Regress√£o exige estrutura estat√≠stica conhecida. |
| **Aplica√ß√µes**       | Otimiza√ß√£o de pre√ßos, testes A/B, marketing online | Previs√£o de vendas, risco de cr√©dito, churn | MAB √© melhor para aprendizado ativo. Regress√£o serve para estimativa e interpreta√ß√£o. |
""", unsafe_allow_html=True)

# Limita√ß√µes
st.markdown("---")
st.markdown("## ‚ö†Ô∏è Limita√ß√µes do Modelo")

st.markdown("""
- Pode ter desempenho fraco no in√≠cio (explora√ß√£o aleat√≥ria),nos primeiros passos ele ainda n√£o tem informa√ß√µes suficientes para saber qual pre√ßo √© melhor. Ele testa aleatoriamente o que pode gerar escolhas ruins no come√ßo.

- Modelos simples como UCB n√£o consideram vari√°veis contextuais, ignora fatores como dia da semana, canal de venda, promo√ß√µes ativas e etc. O pre√ßo ideal em um contexto pode ser diferente em outro contexto.

- Requer n√∫mero razo√°vel de intera√ß√µes para convergir. Se houver poucos dados pode escolher pre√ßos ruins por sorte, se voc√™ s√≥ testou o pre√ßo R$ 9 duas vezes e ele deu lucro alto, o algoritmo ainda n√£o sabe se foi sorte ou se √© o melhor pre√ßo mesmo.

- Assume que as recompensas n√£o mudam com o tempo (estacionariedade),assume que o comportamento do cliente e o lucro por pre√ßo permanecem constantes ao longo do tempo.
""")

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import pickle
import os

# -------------------
# AN√ÅLISE DE DADOS
# -------------------

st.markdown("---")
st.title("üì¶ Vis√£o Geral dos Produtos (SKU) + Dispers√£o de Lucro")

uploaded_file = st.file_uploader("üìÇ Fa√ßa upload da base hist√≥rica (.xlsx)", type=["xlsx"])

if uploaded_file:
    df = pd.read_excel(uploaded_file)

    # Convers√£o segura para colunas num√©ricas
    cols_to_convert = ['price', 'unit_cost', 'demand', 'total_profit', 'total_revenue', 'unit_profit']
    for col in cols_to_convert:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    st.subheader("üìã Pr√©via da Base de Dados")
    st.dataframe(df.head())

    # ---------------------
    # INDICADORES POR PRODUTO
    # ---------------------
    resumo_por_sku = df.groupby(['sku_id', 'sku_name']).agg(
        preco_medio=('price', 'mean'),
        custo_medio=('unit_cost', 'mean'),
        lucro_medio_por_unidade=('unit_profit', 'mean'),
        demanda_total=('demand', 'sum'),
        faturamento_total=('total_revenue', 'sum'),
        lucro_total= ('total_profit', 'sum'),
        dias_registrados=('sku_id', 'count')
    ).reset_index()

    st.subheader("üìä Indicadores Agregados por SKU")
    st.dataframe(resumo_por_sku)

    # ---------------------
    # BOX PLOT DOS LUCROS
    # ---------------------
    st.subheader("üìà Dispers√£o dos Lucros Di√°rios por Produto")

    fig, ax = plt.subplots(figsize=(10, 5))
    sns.boxplot(data=df, x='sku_name', y='total_profit', palette='viridis', ax=ax)
    sns.stripplot(data=df, x='sku_name', y='total_profit', color='#b8860b', alpha=0.8, jitter=0.2, ax=ax)
    ax.set_title("Boxplot + Pontos de Lucro Di√°rio por Produto (SKU)")
    ax.set_ylabel("Lucro por Observa√ß√£o (R$)")
    ax.set_xlabel("Produto (SKU)")
    ax.tick_params(axis='x', rotation=45)
    st.pyplot(fig)

    # ---------------------
    # OTIMIZA√á√ÉO DE PRE√áO COM MAB
    # ---------------------
    st.markdown("---")
    st.markdown("## ü§ñ Otimiza√ß√£o com Multi-Armed Bandits")

# ---------------------
# SELE√á√ÉO DE SKU
# ---------------------
    st.subheader("üîç An√°lise por SKU e Algoritmos MAB")
    sku_opcoes = df['sku_name'].unique()
    sku_selecionado = st.selectbox("Escolha o SKU:", options=sku_opcoes, key="sku_otimizacao")
    df_sku = df[df['sku_name'] == sku_selecionado].copy()

    df_sku['reward'] = df_sku['total_profit']
    df_sku['arm'] = df_sku['price'].round(2)

# Inicialmente define os pre√ßos e bra√ßos com base nos dados reais
    price_options = sorted(df_sku['arm'].unique())
    n_arms = len(price_options)
    arm_map = {price: idx for idx, price in enumerate(price_options)}
    df_sku['arm_index'] = df_sku['arm'].map(arm_map)

    class UCBBandit:
        def __init__(self, n):
            self.n = n
            self.counts = np.zeros(n)
            self.values = np.zeros(n)
            self.total = 0

        def select_arm(self):
            self.total += 1
            if 0 in self.counts:
                return np.argmin(self.counts)
            return np.argmax(self.values + np.sqrt(2 * np.log(self.total) / self.counts))

        def update(self, arm, reward):
            self.counts[arm] += 1
            n = self.counts[arm]
            value = self.values[arm]
            self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward

    class EpsilonGreedyBandit:
        def __init__(self, n, epsilon=0.1):
            self.n = n
            self.epsilon = epsilon
            self.counts = np.zeros(n)
            self.values = np.zeros(n)

        def select_arm(self):
            if np.random.rand() < self.epsilon:
                return np.random.randint(self.n)
            return np.argmax(self.values)

        def update(self, arm, reward):
            self.counts[arm] += 1
            n = self.counts[arm]
            value = self.values[arm]
            self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward

    class ThompsonSamplingBandit:
        def __init__(self, n):
            self.n = n
            self.counts = np.zeros(n)
            self.sum_rewards = np.zeros(n)
            self.sum_squares = np.zeros(n)

        def select_arm(self):
            samples = []
            for i in range(self.n):
                if self.counts[i] < 2:
                    samples.append(np.random.normal(0, 100))
                else:
                    mean = self.sum_rewards[i] / self.counts[i]
                    std = np.sqrt(self.sum_squares[i] / self.counts[i] - mean ** 2 + 1e-6)
                    samples.append(np.random.normal(mean, std))
            return np.argmax(samples)

        def update(self, arm, reward):
            self.counts[arm] += 1
            self.sum_rewards[arm] += reward
            self.sum_squares[arm] += reward ** 2

    ucb = UCBBandit(n_arms)
    eps = EpsilonGreedyBandit(n_arms)
    ts = ThompsonSamplingBandit(n_arms)
    ucb_total = eps_total = ts_total = 0
    ucb_rewards = []
    eps_rewards = []
    ts_rewards = []

    for _, row in df_sku.iterrows():
        ucb_arm = ucb.select_arm()
        reward_ucb = row['reward'] if row['arm_index'] == ucb_arm else 0
        ucb.update(ucb_arm, reward_ucb)
        ucb_total += reward_ucb
        ucb_rewards.append(ucb_total)

        eps_arm = eps.select_arm()
        reward_eps = row['reward'] if row['arm_index'] == eps_arm else 0
        eps.update(eps_arm, reward_eps)
        eps_total += reward_eps
        eps_rewards.append(eps_total)

        ts_arm = ts.select_arm()
        reward_ts = row['reward'] if row['arm_index'] == ts_arm else 0
        ts.update(ts_arm, reward_ts)
        ts_total += reward_ts
        ts_rewards.append(ts_total)

    st.subheader("üìà Lucro Acumulado das Estrat√©gias")
    st.line_chart({"UCB": ucb_rewards, "Epsilon-Greedy": eps_rewards, "Thompson Sampling": ts_rewards})

    st.markdown(f"üí∞ **Lucro total com UCB**: R$ {ucb_total:,.2f}")
    st.markdown(f"üí∞ **Lucro total com Epsilon-Greedy**: R$ {eps_total:,.2f}")
    st.markdown(f"üí∞ **Lucro total com Thompson Sampling**: R$ {ts_total:,.2f}")

    melhor_preco_ucb = price_options[np.argmax(ucb.values)]
    melhor_preco_eps = price_options[np.argmax(eps.values)]
    melhor_preco_ts = price_options[np.argmax(ts.sum_rewards / (ts.counts + 1e-6))]

    st.subheader("üèÜ Pre√ßos Otimizados Aprendidos")
    st.markdown(f"- üéØ Melhor pre√ßo segundo **UCB**: R$ {melhor_preco_ucb}")
    st.markdown(f"- üéØ Melhor pre√ßo segundo **Epsilon-Greedy**: R$ {melhor_preco_eps}")
    st.markdown(f"- üéØ Melhor pre√ßo segundo **TS**: R$ {melhor_preco_ts}")

    def simular_lucro_estimado(df_base, preco):
        media_lucro = df_base[df_base['arm'] == preco]['reward'].mean()
        return media_lucro * len(df_base)

    lucro_ucb_estimado = simular_lucro_estimado(df_sku, melhor_preco_ucb)
    lucro_eps_estimado = simular_lucro_estimado(df_sku, melhor_preco_eps)
    lucro_ts_estimado = simular_lucro_estimado(df_sku, melhor_preco_ts)

    st.subheader("üìà Comparativo Estimado de Estrat√©gias")
    st.markdown(f"- üíº Estimado com pre√ßo **UCB** ({melhor_preco_ucb:.2f}): **R$ {lucro_ucb_estimado:,.2f}**")
    st.markdown(f"- üíº Estimado com pre√ßo **Epsilon-Greedy** ({melhor_preco_eps:.2f}): **R$ {lucro_eps_estimado:,.2f}**")
    st.markdown(f"- üíº Estimado com pre√ßo **Thompson Sampling** ({melhor_preco_ts:.2f}): **R$ {lucro_ts_estimado:,.2f}**")
    st.markdown(f"- üíº **Lucro total hist√≥rico** (todas as vendas): **R$ {df_sku['total_profit'].sum():,.2f}**")
    # ---------------------
    # GR√ÅFICO DE LUCRO M√âDIO POR PRE√áO PRATICADO
    # ---------------------
    st.subheader("üìä Lucro M√©dio por Pre√ßo Praticado no Hist√≥rico")
    lucro_por_preco = df_sku.groupby('arm')['reward'].mean().sort_index()
    st.bar_chart(lucro_por_preco)

    fig3, ax3 = plt.subplots(figsize=(6, 4))
    estrategias = ['Hist√≥rico', 'Estimado UCB', 'Estimado Epsilon','Estimado TS']
    valores = [df_sku['total_profit'].sum(), lucro_ucb_estimado, lucro_eps_estimado,lucro_ts_estimado]
    ax3.bar(estrategias, valores, color=['gray', 'green', 'blue','yellow'])
    ax3.set_ylabel("Lucro Total Estimado (R$)")
    ax3.set_title(" Comparativo Estimado de Estrat√©gias")
    for i, v in enumerate(valores):
        ax3.text(i, v + 0.01 * max(valores), f'R$ {v:,.0f}', ha='center')
    st.pyplot(fig3)

    # ---------------------
    # ANALISES ADICIONAIS DE REGRET
    # ---------------------
    st.subheader("üìâ Regret e Efici√™ncia de Pre√ßo")
    lucro_medio_por_arm = df_sku.groupby('arm')['reward'].mean()
    preco_otimo = lucro_medio_por_arm.idxmax()
    lucro_ideal_unitario = lucro_medio_por_arm.max()
    lucro_real_total = df_sku['reward'].sum()
    T = len(df_sku)
    lucro_ideal_total = T * lucro_ideal_unitario
    regret_total = lucro_ideal_total - lucro_real_total

    st.markdown(f"- üß† **Pre√ßo √≥timo estimado (lucro m√©dio)**: R$ {preco_otimo:.2f}")
    st.markdown(f"- üìâ **Regret acumulado (total perdido por n√£o aplicar o pre√ßo √≥timo)**: R$ {regret_total:,.2f}")

    st.markdown(f"Neste caso, o algoritmo estima que a empresa poderia ter ganho {regret_total:,.2f} a mais se tivesse vendido todas as unidades ao pre√ßo √≥timo de {preco_otimo:.2f}. Esse valor {preco_otimo:.2f} √© o pre√ßo √≥timo estimado com base no hist√≥rico de vendas, definido como o pre√ßo que gerou, em m√©dia, o maior lucro por observa√ß√£o (venda) no dataset.")

    # Gr√°fico de regret acumulado
    regret_series = np.cumsum([lucro_ideal_unitario - r for r in df_sku['reward']])
    st.line_chart(regret_series)

    # Regret m√©dio por pre√ßo praticado
    df_sku['regret_unitario'] = lucro_ideal_unitario - df_sku['reward']
    st.subheader("üìä Regret M√©dio por Pre√ßo Praticado")
    chart_data = df_sku.groupby('price')['regret_unitario'].mean().sort_index()
    st.bar_chart(chart_data)

    st.markdown("Regret m√©dio = (Lucro m√©dio com pre√ßo √≥timo) ‚Äì (Lucro m√©dio com pre√ßo praticado). Aqui podemos medir o quanto de lucro deixamos de ganhar, em m√©dia, ao vender a esse pre√ßo espec√≠fico em vez do pre√ßo √≥timo.")

    # Ranking de SKUs por regret total
    regret_por_sku = df.groupby('sku_name').apply(
        lambda x: (lucro_ideal_unitario - x['total_profit'].mean()) * len(x)
    ).sort_values(ascending=False).reset_index()
    regret_por_sku.columns = ['sku_name', 'regret_estimado']

    st.subheader("üè∑Ô∏è Ranking de Produtos por Regret Estimado")
    st.dataframe(regret_por_sku)

    st.markdown("Este ranking mostra quanto cada produto deixou de lucrar, em compara√ß√£o com o pre√ßo √≥timo estimado.Produtos com maior regret estimado indicam oportunidades de otimiza√ß√£o de pre√ßo.")

# üìä Visualiza√ß√£o de Incerteza (Thompson Sampling)
# ---------------------
    st.subheader("üìâ Incerteza Estimada por Pre√ßo (Thompson Sampling)")

    fig_ts, ax_ts = plt.subplots(figsize=(10, 5))
    x = np.linspace(df_sku['reward'].min() * 0.9, df_sku['reward'].max() * 1.1, 200)

    # Apenas pre√ßos com no m√≠nimo 5 observa√ß√µes
    precos_confiaveis = [i for i in range(n_arms) if ts.counts[i] >= 5]
    colors = plt.cm.viridis(np.linspace(0, 1, len(precos_confiaveis)))

    for j, i in enumerate(precos_confiaveis):
        mean = ts.sum_rewards[i] / ts.counts[i]
        var = ts.sum_squares[i] / ts.counts[i] - mean ** 2
        std = np.sqrt(max(var, 1e-6))
        pdf = (1 / (std * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std) ** 2)
        ax_ts.plot(x, pdf, label=f"{price_options[i]:.2f}", color=colors[j], alpha=0.8)

    ax_ts.set_title("Distribui√ß√£o Posterior das Recompensas Estimadas (TS)")
    ax_ts.set_xlabel("Lucro Estimado")
    ax_ts.set_ylabel("Densidade")
    ax_ts.legend(title="Pre√ßo", bbox_to_anchor=(1.05, 1), loc='upper left')
    st.pyplot(fig_ts)

    st.markdown("""

- Cada curva no gr√°fico representa a distribui√ß√£o da incerteza do lucro estimado para um pre√ßo espec√≠fico.

- Essas curvas foram geradas pelo algoritmo Thompson Sampling, que assume que o lucro de cada pre√ßo segue uma distribui√ß√£o normal com m√©dia e vari√¢ncia estimadas a partir dos dados.

- Os picos das curvas indicam o lucro m√©dio estimado por unidade vendida para cada pre√ßo testado.

- **Curva Estreita**: Algoritmo tem alta confian√ßa no lucro estimado (muitos testes, dados consistentes).  

- **Curva Ampla ou Achatadas**: Alta incerteza sobre o lucro (poucos testes ou alta variabilidade nos resultados).

- Priorizar pre√ßos com:
    - Alta m√©dia estimada 
    - Baixa vari√¢ncia (curvas bem definidas)

""")

    st.markdown("---")
    st.subheader("üîç Compara√ß√£o Visual: Curvas Estreitas vs. Curvas Amplas (TS)")

    # Exemplo fict√≠cio de distribui√ß√µes normais
    import numpy as np
    import matplotlib.pyplot as plt

    x = np.linspace(0, 1000, 500)

    # Curva concentrada (baixa incerteza)
    mu_baixa_inc = 600
    sigma_baixa_inc = 30
    y1 = (1 / (sigma_baixa_inc * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu_baixa_inc) / sigma_baixa_inc) ** 2)

    # Curva ampla (alta incerteza)
    mu_alta_inc = 600
    sigma_alta_inc = 150
    y2 = (1 / (sigma_alta_inc * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu_alta_inc) / sigma_alta_inc) ** 2)

    # Plot
    fig_exemplo, ax_exemplo = plt.subplots(figsize=(10, 4))
    ax_exemplo.plot(x, y1, label="Curva Estreita (Alta Confian√ßa)", color='green')
    ax_exemplo.plot(x, y2, label="Curva Ampla (Alta Incerteza)", color='red')
    ax_exemplo.set_title("Exemplo Comparativo: Distribui√ß√µes Posteriores (TS)")
    ax_exemplo.set_xlabel("Lucro Estimado")
    ax_exemplo.set_ylabel("Densidade")
    ax_exemplo.legend()
    st.pyplot(fig_exemplo)

# ---------------------
# üìã Tabela com Intervalo de Confian√ßa (95%)
# ---------------------
    st.subheader("üìã Estimativas com Intervalo de Confian√ßa (95%) - Thompson Sampling")

    dados_ts = []
    for i, preco in enumerate(price_options):
        if ts.counts[i] >= 5:
            mu = ts.sum_rewards[i] / ts.counts[i]
            var = ts.sum_squares[i] / ts.counts[i] - mu ** 2
            std = np.sqrt(max(var, 1e-6))
            ci_low, ci_up = stats.norm.interval(0.95, loc=mu, scale=std)
            dados_ts.append([preco, mu, std, ci_low, ci_up, int(ts.counts[i])])

    df_ts = pd.DataFrame(dados_ts, columns=[
        "Pre√ßo", "M√©dia Estimada", "Desvio Padr√£o", "CI Inferior", "CI Superior", "N Testes"
    ])

    st.dataframe(df_ts.style.format({
        "Pre√ßo": "{:.2f}",
        "M√©dia Estimada": "{:.2f}",
        "Desvio Padr√£o": "{:.2f}",
        "CI Inferior": "{:.2f}",
        "CI Superior": "{:.2f}"
    }))

    st.markdown("""
A tabela acima mostra, para cada pre√ßo testado, a m√©dia estimada de lucro, o n√≠vel de incerteza (via desvio padr√£o), o intervalo de confian√ßa de 95%, e o n√∫mero de testes realizados.

- Pre√ßo: valor do produto testado.

- M√©dia Estimada: lucro m√©dio observado ao vender com esse pre√ßo.

- Desvio Padr√£o: grau de incerteza nas estimativas. Quanto menor, mais confi√°vel.

- CI Inferior / CI Superior: intervalo de confian√ßa onde, com 95% de certeza, o verdadeiro lucro m√©dio est√° contido.

- N Testes: n√∫mero de vezes que o pre√ßo foi testado (impacta a confiabilidade).

- Mesmo que a m√©dia estimada pare√ßa alta, um desvio padr√£o elevado pode indicar que √© um resultado influenciado por variabilidade/sorte.


    """)
